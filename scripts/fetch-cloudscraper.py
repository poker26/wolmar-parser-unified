#!/usr/bin/env python3

import sys
import os
import time
import json
from datetime import datetime
import cloudscraper
import requests
from bs4 import BeautifulSoup

def fetch_with_cloudscraper(category_id='252', finished=True):
    print('â˜ï¸  Using cloudscraper for Cloudflare bypass...')
    
    url = f"https://meshok.net/good/{category_id}{'?opt=2' if finished else ''}"
    print(f"ðŸ“„ Fetching: {url}")
    
    try:
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ cloudscraper ÑÐµÑÑÐ¸ÑŽ
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False
            }
        )
        
        # Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
            'DNT': '1'
        }
        
        print('â³ Making request with cloudscraper...')
        response = scraper.get(url, headers=headers, timeout=30)
        
        print(f'ðŸ“Š Status code: {response.status_code}')
        print(f'ðŸ“Š Response size: {len(response.text) / 1024:.2f} KB')
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð° Cloudflare
        if 'Just a moment' in response.text or 'ÐžÐ´Ð¸Ð½ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚' in response.text:
            print('âš ï¸  Cloudflare challenge detected')
        else:
            print('âœ… No Cloudflare challenge detected')
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
        timestamp = datetime.now().strftime('%Y-%m-%dT%H-%M-%S-%fZ')
        filename = f"cloudscraper_good{category_id}_opt{'2' if finished else '1'}_{timestamp}.html"
        filepath = os.path.join('data', filename)
        
        os.makedirs('data', exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(response.text)
        
        print(f'âœ… Saved to: {filename}')
        
        # ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ÐŸÐ¾Ð¸ÑÐº Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°
        title = soup.find('title')
        if title:
            print(f'ðŸ“‹ Page title: {title.text}')
        
        # ÐŸÐ¾Ð¸ÑÐº ÑÑÑ‹Ð»Ð¾Ðº Ð½Ð° Ð»Ð¾Ñ‚Ñ‹
        item_links = soup.find_all('a', href=lambda x: x and '/item/' in x)
        print(f'ðŸ”— Item links found: {len(item_links)}')
        
        if len(item_links) > 0:
            print('ðŸŽ‰ Successfully obtained auction data with cloudscraper!')
            
            # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÑÑ‹Ð»Ð¾Ðº
            print('ðŸ“‹ First 5 item links:')
            for i, link in enumerate(item_links[:5]):
                href = link.get('href')
                text = link.get_text().strip()[:50]
                print(f'   {i + 1}. {text}... -> {href}')
        else:
            print('âš ï¸  No auction links found')
        
        # ÐŸÐ¾Ð¸ÑÐº Ñ†ÐµÐ½
        price_matches = []
        for text in soup.stripped_strings:
            if 'â‚½' in text or 'Ñ€ÑƒÐ±' in text:
                price_matches.append(text)
        
        if price_matches:
            print(f'ðŸ’° Prices found: {len(price_matches)}')
            print('ðŸ“‹ Sample prices:')
            for i, price in enumerate(price_matches[:3]):
                print(f'   {i + 1}. {price}')
        
        # ÐŸÐ¾Ð¸ÑÐº Ñ‚Ð°Ð±Ð»Ð¸Ñ†
        tables = soup.find_all('table')
        print(f'ðŸ“Š Tables found: {len(tables)}')
        
        # ÐŸÐ¾Ð¸ÑÐº Ñ„Ð¾Ñ€Ð¼
        forms = soup.find_all('form')
        print(f'ðŸ“ Forms found: {len(forms)}')
        
        # ÐŸÐ¾Ð¸ÑÐº JSON Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² script Ñ‚ÐµÐ³Ð°Ñ…
        scripts = soup.find_all('script')
        json_found = False
        for script in scripts:
            if script.string and ('{' in script.string and '}' in script.string):
                json_found = True
                break
        
        if json_found:
            print('ðŸ“œ JSON data found in scripts')
        else:
            print('ðŸ“œ No JSON data found in scripts')
        
    except Exception as e:
        print(f'âŒ Error: {e}')

if __name__ == '__main__':
    category_id = sys.argv[1] if len(sys.argv) > 1 else '252'
    finished = sys.argv[2] != 'false' if len(sys.argv) > 2 else True
    fetch_with_cloudscraper(category_id, finished)
